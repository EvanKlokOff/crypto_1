* Сообщение о значении св Y пришло, какая тогда информация в конкретном сообщении о случайной величине X ?
  1. $J(\mathcal{X}=x_i, \mathcal{Y}=y_j)$ = $-log(P(\mathcal{X}=x_i, \mathcal{Y}=y_j))$

* Информация о значении св X, без сообщения об Y

  2. $J(\mathcal{X}=x_i) = -log(P(\mathcal{X}=x_i))$

* **разность 2 - 1**
  
  * $-log(P(\mathcal{X}=x_i)) + log(P(\mathcal{X}=x_i, \mathcal{Y}=y_j))$
  
  * **Эта разность обозначается как $J(\mathcal{X}=x_i \leftarrow \mathcal{Y}=y_j)$ и представляет собой кол-во инфы в сообщении $\mathcal{Y}=y_j$ о том, что св $\mathcal{X} = x_i$**

#### Немного преобразуем


$J(A\leftarrow B)$= $-log(P(A))$ + $log(P(A|B))$ =

* $P(A|B) = \frac{P(A B)}{P(B)}$ 

$-log(P(A)) - log(P(B)) + log(P(AB)) =$

* Видим что формула симитричная

$J(B\leftarrow A)$ =

* Загоним всё под один логорифм
  
$log(\frac{P(A B)}{P(A)\cdot P(B)})$

* Видим, что $J(B\leftarrow A) = J(A\leftarrow B)$ зависит от того какое A и B мы возьмем(В нашем случае от индексо i и j, т.к они определяют значение случайных величин X и Y соответственно)

* Из этого следует, что $J(\mathcal{X}=x_i \Leftrightarrow \mathcal{Y}=y_j)$ - это функция от Случайной величины $(X,Y)$, т.е тоже случайная величина
  
  * Пишем теперь двойную стрелку, т.к убедилилсь, что J$(X\leftarrow Y) = J(Y\leftarrow X)$

## ОПР(Взаимная информация)

$I(X\leftrightarrow Y)$ = $M[J(\mathcal{X}=x_i \Leftrightarrow \mathcal{Y}=y_j)]$ - взаимная информация между случайными величинами X и Y.

* это среднее кол-во информации, которое мы получаем в сообщение о значении одной случайной о значении другой случайной св

* Смотрим на значение одной св и получаем инфу о значении другой

# Теорема о взаимной информации(Главная часть билета)

1. $0 \le I(X\leftrightarrow Y)$

2. $I(X\leftrightarrow Y) = H(X) + H(Y) - H(X,Y)$

## Д-ВО

$I(X\leftrightarrow Y) = \sum_{i=1}^n\sum_{j=1}^n\lgroup log(\frac{P(X=x_i, Y=y_j)}{P(X=x_i)\cdot P(Y=y_j)}) \cdot P(X=x_i, Y=y_j)\rgroup$


### Д-ЖЕМ 2 часть теоремы 

* Теперь разделываем эту формулу на части

$I(X\leftrightarrow Y) = \sum_{i=1}^n\sum_{j=1}^n\lgroup log(P(X=x_i, Y=y_j) - log(P(X=x_i)\cdot P(Y=y_j))) \cdot P(X=x_i, Y=y_j)\rgroup$

* $log(P(X=x_i, Y = y_j)) \cdot P(X=x_i, Y=y_j) = - H(X,Y)$ по опр H(X,Y)

* $-\sum_{i=1}^n\sum_{j=1}^n log(P(X=x_i))\cdot P(X=x_i, Y = y_j)$ = 
  
  *  $-\sum_{i=1}^n \lgroup log(P(X=x_i))\sum_{j=1}^n  P(X=x_i, Y = y_j)\rgroup$
  
  *  $-\sum_{i=1}^n \lgroup log(P(X=x_i))P(X=x_i)\rgroup$ = H(X)

* $-\sum_{i=1}^n\sum_{j=1}^n log(P(Y=y_j))\cdot P(X=x_i, Y = y_j)$ = 
  
  *  $-\sum_{i=1}^n \lgroup log(P(Y=y_j))P(Y=y_j)\rgroup$ = H(Y)

$\blacksquare$

### Д-ЖЕМ 1 часть теоремы

$\measuredangle$ $-I(X\leftrightarrow Y)$ =


$-\sum_{i=1}^n\sum_{j=1}^n\lgroup log(\frac{P(X=x_i, Y=y_j)}{P(X=x_i)\cdot P(Y=y_j)}) \cdot P(X=x_i, Y=y_j)\rgroup$ =

$\sum_{i=1}^n\sum_{j=1}^n\lgroup log(\frac{P(X=x_i)\cdot P(Y=y_j)}{P(X=x_i, Y=y_j)}) \cdot P(X=x_i, Y=y_j)\rgroup$ =

$log(e)\sum_{i=1}^n\sum_{j=1}^n\lgroup ln(\frac{P(X=x_i)\cdot P(Y=y_j)}{P(X=x_i, Y=y_j)}) \cdot P(X=x_i, Y=y_j)\rgroup \le$ 

* вспоминаем, что $ln(x) \le x-1$
 
$log(e)\sum_{i=1}^n\sum_{j=1}^n\lgroup (\frac{P(X=x_i)\cdot P(Y=y_j)}{P(X=x_i, Y=y_j)} - 1) \cdot P(X=x_i, Y=y_j)\rgroup$ =

* раскрываем скобки
  
$log(e)\lgroup \sum_{i=1}^n\sum_{j=1}^n\lgroup (P(X=x_i)\cdot P(Y=y_j)) - \sum_{i=1}^n\sum_{j=1}^n P(X=x_i, Y=y_j) \rgroup$ = 0

* $\sum_{i=1}^n\sum_{j=1}^n P(X=x_i, Y=y_j)$ = 1

* $\sum_{i=1}^n\sum_{j=1}^n\lgroup (P(X=x_i)\cdot P(Y=y_j)) = 1$

Получаем, что $- I(X\leftrightarrow Y) \le 0$

$\blacksquare$

### Следствие 

Когда X,Y - независимые случайные величины,то $I(X\leftrightarrow Y) = 0$