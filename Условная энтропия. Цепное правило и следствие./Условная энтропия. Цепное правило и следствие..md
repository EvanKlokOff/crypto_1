* Есть 2 случайные величины $\mathcal{X, Y}$

* Пусть известно что, $\mathcal{Y} = y$

* Тогда $H(\mathcal{X}|\mathcal{Y}=y) = -\sum_{i=1}^n \lgroup p(\mathcal{X} = x_i| \mathcal{Y}=y) \cdot log(P(\mathcal{x}=x_i|\mathcal{Y}=y))\rgroup$

* $H(\mathcal{X}|\mathcal{Y}=y)$ - это среднее кол-во информации в сообщении о значении X, если до этого было получено сообщение Y = y
  
* $H(\mathcal{X}|\mathcal{Y}=y)$ - это функция от св Y, т.е тоже СВ
### ОПР(Условной энтропии)

**Условная энтропия X при условии Y** - мат ожидание $H(\mathcal{X}|\mathcal{Y}=y)$
$H(X|Y) =$

$M[H(\mathcal{X}|\mathcal{Y}=y)] =$

$-\sum_{j=1}^k\lgroup P(\mathcal{Y} = y_j) \cdot \sum_{i=1}^n \lgroup p(\mathcal{X} = x_i| \mathcal{Y}=y_j) \cdot log(P(\mathcal{x}=x_i|\mathcal{Y}=y_j))\rgroup$ =

* по формуле совместной вероятности $P(x,y) = P()$

$-\sum_{j=1}^k \lgroup \sum_{i=1}^n \lgroup P(\mathcal{X} = x_i, \mathcal{Y} = y_j) \cdot log(P(\mathcal{x}=x_i|\mathcal{Y}=y_j))\rgroup$

### Замечание 
* Если X, Y - независимые, тогда $\forall i,j: P(\mathcal{X} = x_i | \mathcal{Y} = y_j) = P(\mathcal{X} = x_i)$

* $\sum_{j=1}^k P(\mathcal{X}=x_i, \mathcal{Y}=y_j) = P(\mathcal{X} = x_i)$

* Получаем что H(X|Y) = H(X)

### Можно обобщить определение условной энтропии на 3  и более случайных величины 

* $H(\mathcal{X},\mathcal{Y}) = - \sum_{i=1}^n \lgroup \sum_{j=1}^k P(\mathcal{X} = x_i, \mathcal{Y} = y_i) \cdot log (P(\mathcal{X}=x_i, \mathcal{Y}=y_j))\rgroup$

* $H(\mathcal{X},\mathcal{Y} | \mathcal{Z}) = - \sum_{i=1}^n \lgroup \sum_{j=1}^k \lgroup \sum_{t = 1}^m\lgroup P(\mathcal{X} = x_i, \mathcal{Y} = y_i, \mathcal{Z}=z_t) \cdot log (P(\mathcal{X}=x_i, \mathcal{Y}=y_j | \mathcal{Z}=z_t ))\rgroup \rgroup$

## Цепное правило(Главная теорема билета)

$H(X, Y)$ =

$H(X) + H(Y|X)$ = 

$H(Y) + H(X|Y)$

### Для случая с тремя случайными величинами

$H(X, Y | Z)$ =

$H(X|Z) + H(Y|X, Z)$ = 

$H(Y|Z) + H(X|Y, Z)$



### Д-ВО

$P(A\cdot B) = P(B) \cdot P(A|B)$

**для обычной энтропии**

Тогда получаем $log(P(A B)) = log(P(B)) + log(P(A|B))$
* $log(P(B))$ -превращается в энтропию Y
* $log(P(A|B))$ - превращается в условную энтропию X при условии Y
* мы рассматриваем $log(P(X=x_i, Y = y_j))$ из опр H(X,Y)
 
**Для условной энтропии**

* мы рассматриваем $log(P(X=x_i, Y = y_j|Z=z_t))$ из опр H(X,Y|Z)

$P(A B | C)$ = 

$\frac{P(A B C)}{P(C)}$ = 

$\frac{P(B C) \cdot P(A|BC) }{P(C)}$ =

$\frac{P(C) \cdot P(B|C) \cdot P(A|BC)}{P(C)}$ = 

$P(B|C) \cdot P(A|BC)$

$log P(X=x_i, Y=y_j|Z=z_t) = log(X=x_i | Y=y_j, Z=z_t) + log(Y=y_j|Z=z_t)$

* $log(X=x_i | Y=y_j, Z=z_t)$ превращается в $H(X|Y,Z)$
* $log(Y=y_j|Z=z_t)$ превращается в H(Y|Z)

$\blacksquare$

## Следствие из цепного правила и замечания

Если X, Y - независимы, то H(X,Y) = H(X) + H(Y)