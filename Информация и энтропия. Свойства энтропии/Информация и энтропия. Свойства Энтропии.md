# Про информацию

Информация всегда содержится в сообщении

## Как измерить информацию

### Первое предположение:

**Кол-во инфы в сообщение примерно пропорционально длине сообщения**. К сожалению это не всегда так, нужно просто послушать пьяные бредни алкашей и всё станет понятно - Слов дохуя, а смысла нихуя

### Второе предположение

**Кол-во информации в сообщении зависит от вероятности события**. Чем меньше вероятность возникновения события, тем больше информации несет сообщение. 

### ОПР(Кол-во информации)
Пусть **кол-во информации** в сообщении "Произошло A" J(A) = J(произошло A) -это функция от вероятности события A.

* $J(A) = J(P(A)) \Rightarrow J:[0,1] \rightarrow \Reals^+ \cup \{0\}$
* J - это непрерывная функция(т.е если мы чуть-чуть изменим вероятность события A, то кол-во информации тоже изменится на чуть-чуть)
* J - монотонно убывающая функция
* Если $P(A) = 1 \Rightarrow J(A)=0$
* Если A и B - независимые, то $J(A \cdot B) = J(A) + J(B)$
* Если $P(A) =\frac{1}{2}$, то $J(A)=1$ 

# Теорема об информации

1. $\forall p,q \in [0,1]: J(p\cdot q) = J(p)+J(q)$
2. J(p) - непрерывная
3. $J(\frac{1}{2}) = 1$

Тогда $J(p) = -log(p)$

## Д-ВО
для 1ого свойства проведем индукцию по n и получим свойство 1')   

1') $J(p^n) = n\cdot J(p)$

* сделаем замену $p^n = q \rightarrow n = q^{\frac{1}{n}}$

1'')$\frac{1}{n} \cdot J(p) = J(q^{\frac{1}{n}})$ 

* Получили свойство 1'') для чисел степеней обратных натуральным

$\measuredangle J(p^\frac{m}{n}) = \frac{1}{n}\cdot J(p^m) = \frac{m}{n}\cdot J(p)$

* Получили аналог свойства 1) для всех натуральных чисел

$J(\frac{1}{2}^\frac{m}{n}) =  \frac{m}{n} \cdot J(\frac{1}{2}) = \frac{m}{n}$

Теперь покажем, что $J(p)$ это именно $-log(p)$

* т.к $-log(p)$ - это действительное число, то можно построить 2 последовательности рациональных чисел, которые будут стремиться к нему сверху и снизу

$\{\frac{m_i}{n_i}\}_{i=0}^{\infty}\nearrow -log(p)$


$\{\frac{u_n}{v_i}\}_{i=0}^{\infty}\searrow -log(p)$

Тогда $\forall i: \frac{m_i}{n_i} < log(p) < \frac{u_n}{v_i}\Rightarrow$ 

$-\frac{m_i}{n_i} > log(p) > -\frac{u_n}{v_i} \Rightarrow$

$2^{-\frac{m_i}{n_i}} > p > 2^{-\frac{u_n}{v_i}} \Rightarrow$

* т.к J(p) - убываюящая, то меняем знак неравенства

$J(2^{-\frac{m_i}{n_i}}) < J(p) < J(2^{-\frac{u_n}{v_i}}) \Rightarrow$


$\frac{m_i}{n_i} < J(p) < \frac{u_n}{v_i} \Rightarrow$

* Делаем предельный переход по i

$-log(p) = \lim\limits_{i \to \infty}\frac{m_i}{n_i} \le J(p) \le \lim\limits_{i \to \infty}\frac{u_i}{v_i} = -log(p)$

$\blacksquare$

### ОПР(Стохастический вектор)

$(x_1, x_2, \cdots, x_n) = \vec{x}$ - **стохастический вектор**, если верно:
* $\vec{x} \in \Reals^n$
* $\forall i: x_i \ge 0$
* $\sum_{i=1}^n x_i = 1$

### ОПР(Энтропия)

Пусть $\mathcal{X}$ - дискретная случайная величина. 

Тогда $J(\mathcal{X} = x_i) = -log(P(\mathcal{X} = x_i))$

* $J(p)$ - это функция от случайной величины $\Rightarrow$ тоже случайная величина

**Энтропия случайной величины** $\mathcal{X}$ - Это мат.ожидание кол-ва информации

* $H(x) = M[J(x)]$ 

* $H(x) = \sum_{i=0}^n p_i \cdot J(p_i) = -\sum_{i=0}^n p_i \cdot log(p_i)$

* $H(x) = H(p_1, p_2, ... p_n) = H(\vec{p})$

* слагаемые вида $0\cdot log(0)$ можно не учитывать в подсчете энтропии


## Cвойства энтропии
1. $H(p_1, p_2, ... p_n, 0) =  H(p_1, p_2, ... p_n)$

2. Пусть $\sigma \in S_n$. Тогда $H(p_{\sigma(1)}, p_{\sigma(2)}, ... p_{\sigma(n)}) = H(\vec{p})$
   * $\sigma \in S_n$ - это перестановки длины n
   * Т.е как бы мы не переставляли аргументы функции H(p), результат её не изменится
3. $0\le H(p_1, p_2, ... p_n)\le log(n)=H(\frac{1}{n}, \frac{1}{n}, ... \frac{1}{n})$ **правое равенство достигается при равномерном распределении**

4. $H(p_1, ... p_n) = H(p_1, \cdots, p_{n-2},p_{n-1} + p_n)+H(p_{n-1}+p_n) \cdot H(\frac{p_{n-1}}{p_{n-1} + p_n}, \frac{p_{n}}{p_{n-1} + p_n})$
   * Для д-ва просто подставь все в формулы

### Д-ВО( $\le$ 3-его свойства)
Сначала покажем, что $ln(x) \le x-1$

* Графики имеют единственную точку пересечения в (1, 0)

Сравним производные
* $(ln(x))' = \frac{1}{x}$
* $(x-1)' = 1$
* Видим, что $(ln(x))' = \frac{1}{x} < 1 = (x-1)' (\forall x>1)$
* При $(0<x<1): 1<\frac{1}{x}$

$\measuredangle H(p)-log(n)$ =

$-\sum_{i=1}^n p_i \cdot log(p_i)$ - $log(n)$ = 

* $\sum_{i=1}^n p_i = 1$

$-\sum_{i=1}^n p_i \cdot log(p_i)$ - $\sum_{i=1}^n p_i \cdot log(n)$ = 

$-\sum_{i=1}^n \cdot p_i \cdot(log(p_i) + log(n))$ =

$\sum_{i=1}^n \cdot p_i \cdot(log(\frac{1}{n\cdot p_i}))$ =

* $log(x) = \frac{ln(x)}{ln(2)}$
* $ln(x) = \frac{log(x)}{log(e)}$

$log(e) \cdot \sum_{i=1}^n p_i \cdot ln(\frac{1}{n \cdot p_i}) \le$

$log(e) \cdot \sum_{i=1}^n p_i \cdot (\frac{1}{n \cdot p_i} - 1) =$
  
$log(e)(\sum_{i=1}^n \frac{1}{n} - \sum_{i=n}^np_i) = 0$

$\Rightarrow H(\vec{p}) - log(n) \le 0$

Равенство будет при равномерном распределении. В этом случае $p_i=\frac{1}{n}$ и тогда $log(e) \cdot \sum_{i=1}^n p_i \cdot (\frac{1}{n \cdot p_i} - 1) = 0$